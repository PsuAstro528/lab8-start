<!DOCTYPE html>
<HTML lang = "en">
<HEAD>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  
  

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>

  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  
<style>
pre.hljl {
    border: 1px solid #ccc;
    margin: 5px;
    padding: 5px;
    overflow-x: auto;
    color: rgb(68,68,68); background-color: rgb(251,251,251); }
pre.hljl > span.hljl-t { }
pre.hljl > span.hljl-w { }
pre.hljl > span.hljl-e { }
pre.hljl > span.hljl-eB { }
pre.hljl > span.hljl-o { }
pre.hljl > span.hljl-k { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kc { color: rgb(59,151,46); font-style: italic; }
pre.hljl > span.hljl-kd { color: rgb(214,102,97); font-style: italic; }
pre.hljl > span.hljl-kn { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kp { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kr { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kt { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-n { }
pre.hljl > span.hljl-na { }
pre.hljl > span.hljl-nb { }
pre.hljl > span.hljl-nbp { }
pre.hljl > span.hljl-nc { }
pre.hljl > span.hljl-ncB { }
pre.hljl > span.hljl-nd { color: rgb(214,102,97); }
pre.hljl > span.hljl-ne { }
pre.hljl > span.hljl-neB { }
pre.hljl > span.hljl-nf { color: rgb(66,102,213); }
pre.hljl > span.hljl-nfm { color: rgb(66,102,213); }
pre.hljl > span.hljl-np { }
pre.hljl > span.hljl-nl { }
pre.hljl > span.hljl-nn { }
pre.hljl > span.hljl-no { }
pre.hljl > span.hljl-nt { }
pre.hljl > span.hljl-nv { }
pre.hljl > span.hljl-nvc { }
pre.hljl > span.hljl-nvg { }
pre.hljl > span.hljl-nvi { }
pre.hljl > span.hljl-nvm { }
pre.hljl > span.hljl-l { }
pre.hljl > span.hljl-ld { color: rgb(148,91,176); font-style: italic; }
pre.hljl > span.hljl-s { color: rgb(201,61,57); }
pre.hljl > span.hljl-sa { color: rgb(201,61,57); }
pre.hljl > span.hljl-sb { color: rgb(201,61,57); }
pre.hljl > span.hljl-sc { color: rgb(201,61,57); }
pre.hljl > span.hljl-sd { color: rgb(201,61,57); }
pre.hljl > span.hljl-sdB { color: rgb(201,61,57); }
pre.hljl > span.hljl-sdC { color: rgb(201,61,57); }
pre.hljl > span.hljl-se { color: rgb(59,151,46); }
pre.hljl > span.hljl-sh { color: rgb(201,61,57); }
pre.hljl > span.hljl-si { }
pre.hljl > span.hljl-so { color: rgb(201,61,57); }
pre.hljl > span.hljl-sr { color: rgb(201,61,57); }
pre.hljl > span.hljl-ss { color: rgb(201,61,57); }
pre.hljl > span.hljl-ssB { color: rgb(201,61,57); }
pre.hljl > span.hljl-nB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nbB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nfB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nh { color: rgb(59,151,46); }
pre.hljl > span.hljl-ni { color: rgb(59,151,46); }
pre.hljl > span.hljl-nil { color: rgb(59,151,46); }
pre.hljl > span.hljl-noB { color: rgb(59,151,46); }
pre.hljl > span.hljl-oB { color: rgb(102,102,102); font-weight: bold; }
pre.hljl > span.hljl-ow { color: rgb(102,102,102); font-weight: bold; }
pre.hljl > span.hljl-p { }
pre.hljl > span.hljl-c { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-ch { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cm { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cp { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cpB { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cs { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-csB { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-g { }
pre.hljl > span.hljl-gd { }
pre.hljl > span.hljl-ge { }
pre.hljl > span.hljl-geB { }
pre.hljl > span.hljl-gh { }
pre.hljl > span.hljl-gi { }
pre.hljl > span.hljl-go { }
pre.hljl > span.hljl-gp { }
pre.hljl > span.hljl-gs { }
pre.hljl > span.hljl-gsB { }
pre.hljl > span.hljl-gt { }
</style>



  <style type="text/css">
  @font-face {
  font-style: normal;
  font-weight: 300;
}
@font-face {
  font-style: normal;
  font-weight: 400;
}
@font-face {
  font-style: normal;
  font-weight: 600;
}
html {
  font-family: sans-serif; /* 1 */
  -ms-text-size-adjust: 100%; /* 2 */
  -webkit-text-size-adjust: 100%; /* 2 */
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block; /* 1 */
  vertical-align: baseline; /* 2 */
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit; /* 1 */
  font: inherit; /* 2 */
  margin: 0; /* 3 */
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button; /* 2 */
  cursor: pointer; /* 3 */
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box; /* 1 */
  padding: 0; /* 2 */
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield; /* 1 */
  -moz-box-sizing: content-box;
  -webkit-box-sizing: content-box; /* 2 */
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0; /* 1 */
  padding: 0; /* 2 */
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  font-family: monospace, monospace;
  font-size : 0.8em;
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
thead th {
    border-bottom: 1px solid black;
    background-color: white;
}
tr:nth-child(odd){
  background-color: rgb(248,248,248);
}


/*
* Skeleton V2.0.4
* Copyright 2014, Dave Gamache
* www.getskeleton.com
* Free to use under the MIT license.
* http://www.opensource.org/licenses/mit-license.php
* 12/29/2014
*/
.container {
  position: relative;
  width: 100%;
  max-width: 960px;
  margin: 0 auto;
  padding: 0 20px;
  box-sizing: border-box; }
.column,
.columns {
  width: 100%;
  float: left;
  box-sizing: border-box; }
@media (min-width: 400px) {
  .container {
    width: 85%;
    padding: 0; }
}
@media (min-width: 550px) {
  .container {
    width: 80%; }
  .column,
  .columns {
    margin-left: 4%; }
  .column:first-child,
  .columns:first-child {
    margin-left: 0; }

  .one.column,
  .one.columns                    { width: 4.66666666667%; }
  .two.columns                    { width: 13.3333333333%; }
  .three.columns                  { width: 22%;            }
  .four.columns                   { width: 30.6666666667%; }
  .five.columns                   { width: 39.3333333333%; }
  .six.columns                    { width: 48%;            }
  .seven.columns                  { width: 56.6666666667%; }
  .eight.columns                  { width: 65.3333333333%; }
  .nine.columns                   { width: 74.0%;          }
  .ten.columns                    { width: 82.6666666667%; }
  .eleven.columns                 { width: 91.3333333333%; }
  .twelve.columns                 { width: 100%; margin-left: 0; }

  .one-third.column               { width: 30.6666666667%; }
  .two-thirds.column              { width: 65.3333333333%; }

  .one-half.column                { width: 48%; }

  /* Offsets */
  .offset-by-one.column,
  .offset-by-one.columns          { margin-left: 8.66666666667%; }
  .offset-by-two.column,
  .offset-by-two.columns          { margin-left: 17.3333333333%; }
  .offset-by-three.column,
  .offset-by-three.columns        { margin-left: 26%;            }
  .offset-by-four.column,
  .offset-by-four.columns         { margin-left: 34.6666666667%; }
  .offset-by-five.column,
  .offset-by-five.columns         { margin-left: 43.3333333333%; }
  .offset-by-six.column,
  .offset-by-six.columns          { margin-left: 52%;            }
  .offset-by-seven.column,
  .offset-by-seven.columns        { margin-left: 60.6666666667%; }
  .offset-by-eight.column,
  .offset-by-eight.columns        { margin-left: 69.3333333333%; }
  .offset-by-nine.column,
  .offset-by-nine.columns         { margin-left: 78.0%;          }
  .offset-by-ten.column,
  .offset-by-ten.columns          { margin-left: 86.6666666667%; }
  .offset-by-eleven.column,
  .offset-by-eleven.columns       { margin-left: 95.3333333333%; }

  .offset-by-one-third.column,
  .offset-by-one-third.columns    { margin-left: 34.6666666667%; }
  .offset-by-two-thirds.column,
  .offset-by-two-thirds.columns   { margin-left: 69.3333333333%; }

  .offset-by-one-half.column,
  .offset-by-one-half.columns     { margin-left: 52%; }

}
html {
  font-size: 62.5%; }
body {
  font-size: 1.5em; /* currently ems cause chrome bug misinterpreting rems on body element */
  line-height: 1.6;
  font-weight: 400;
  font-family: "Raleway", "HelveticaNeue", "Helvetica Neue", Helvetica, Arial, sans-serif;
  color: #222; }
h1, h2, h3, h4, h5, h6 {
  margin-top: 0;
  margin-bottom: 2rem;
  font-weight: 300; }
h1 { font-size: 3.6rem; line-height: 1.2;  letter-spacing: -.1rem;}
h2 { font-size: 3.4rem; line-height: 1.25; letter-spacing: -.1rem; }
h3 { font-size: 3.2rem; line-height: 1.3;  letter-spacing: -.1rem; }
h4 { font-size: 2.8rem; line-height: 1.35; letter-spacing: -.08rem; }
h5 { font-size: 2.4rem; line-height: 1.5;  letter-spacing: -.05rem; }
h6 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }

p {
  margin-top: 0; }
a {
  color: #1EAEDB; }
a:hover {
  color: #0FA0CE; }
.button,
button,
input[type="submit"],
input[type="reset"],
input[type="button"] {
  display: inline-block;
  height: 38px;
  padding: 0 30px;
  color: #555;
  text-align: center;
  font-size: 11px;
  font-weight: 600;
  line-height: 38px;
  letter-spacing: .1rem;
  text-transform: uppercase;
  text-decoration: none;
  white-space: nowrap;
  background-color: transparent;
  border-radius: 4px;
  border: 1px solid #bbb;
  cursor: pointer;
  box-sizing: border-box; }
.button:hover,
button:hover,
input[type="submit"]:hover,
input[type="reset"]:hover,
input[type="button"]:hover,
.button:focus,
button:focus,
input[type="submit"]:focus,
input[type="reset"]:focus,
input[type="button"]:focus {
  color: #333;
  border-color: #888;
  outline: 0; }
.button.button-primary,
button.button-primary,
input[type="submit"].button-primary,
input[type="reset"].button-primary,
input[type="button"].button-primary {
  color: #FFF;
  background-color: #33C3F0;
  border-color: #33C3F0; }
.button.button-primary:hover,
button.button-primary:hover,
input[type="submit"].button-primary:hover,
input[type="reset"].button-primary:hover,
input[type="button"].button-primary:hover,
.button.button-primary:focus,
button.button-primary:focus,
input[type="submit"].button-primary:focus,
input[type="reset"].button-primary:focus,
input[type="button"].button-primary:focus {
  color: #FFF;
  background-color: #1EAEDB;
  border-color: #1EAEDB; }
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea,
select {
  height: 38px;
  padding: 6px 10px; /* The 6px vertically centers text on FF, ignored by Webkit */
  background-color: #fff;
  border: 1px solid #D1D1D1;
  border-radius: 4px;
  box-shadow: none;
  box-sizing: border-box; }
/* Removes awkward default styles on some inputs for iOS */
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea {
  -webkit-appearance: none;
     -moz-appearance: none;
          appearance: none; }
textarea {
  min-height: 65px;
  padding-top: 6px;
  padding-bottom: 6px; }
input[type="email"]:focus,
input[type="number"]:focus,
input[type="search"]:focus,
input[type="text"]:focus,
input[type="tel"]:focus,
input[type="url"]:focus,
input[type="password"]:focus,
textarea:focus,
select:focus {
  border: 1px solid #33C3F0;
  outline: 0; }
label,
legend {
  display: block;
  margin-bottom: .5rem;
  font-weight: 600; }
fieldset {
  padding: 0;
  border-width: 0; }
input[type="checkbox"],
input[type="radio"] {
  display: inline; }
label > .label-body {
  display: inline-block;
  margin-left: .5rem;
  font-weight: normal; }
ul {
  list-style: circle; }
ol {
  list-style: decimal; }
ul ul,
ul ol,
ol ol,
ol ul {
  margin: 1.5rem 0 1.5rem 3rem;
  font-size: 90%; }
li > p {margin : 0;}
th,
td {
  padding: 12px 15px;
  text-align: left;
  border-bottom: 1px solid #E1E1E1; }
th:first-child,
td:first-child {
  padding-left: 0; }
th:last-child,
td:last-child {
  padding-right: 0; }
button,
.button {
  margin-bottom: 1rem; }
input,
textarea,
select,
fieldset {
  margin-bottom: 1.5rem; }
pre,
blockquote,
dl,
figure,
table,
p,
ul,
ol,
form {
  margin-bottom: 1.0rem; }
.u-full-width {
  width: 100%;
  box-sizing: border-box; }
.u-max-full-width {
  max-width: 100%;
  box-sizing: border-box; }
.u-pull-right {
  float: right; }
.u-pull-left {
  float: left; }
hr {
  margin-top: 3rem;
  margin-bottom: 3.5rem;
  border-width: 0;
  border-top: 1px solid #E1E1E1; }
.container:after,
.row:after,
.u-cf {
  content: "";
  display: table;
  clear: both; }

pre {
  display: block;
  padding: 9.5px;
  margin: 0 0 10px;
  font-size: 13px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre.hljl {
  margin: 0 0 10px;
  display: block;
  background: #f5f5f5;
  border-radius: 4px;
  padding : 5px;
}

pre.output {
  background: #ffffff;
}

pre.code {
  background: #ffffff;
}

pre.julia-error {
  color : red
}

code,
kbd,
pre,
samp {
  font-family: Menlo, Monaco, Consolas, "Courier New", monospace;
  font-size: 0.9em;
}


@media (min-width: 400px) {}
@media (min-width: 550px) {}
@media (min-width: 750px) {}
@media (min-width: 1000px) {}
@media (min-width: 1200px) {}

h1.title {margin-top : 20px}
img {max-width : 100%}
div.title {text-align: center;}

  </style>
</HEAD>

<BODY>
  <div class ="container">
    <div class = "row">
      <div class = "col-md-12 twelve columns">
        <div class="title">
          
          
          
        </div>

        <h1>Astro 528: High-performance Computing for Astrophysics</h1>
<h2>Lab 8, Exercise 2</h2>
<h2>GPU Programming:  Writing Custom Kernels</h2>

<h3>Why a Jupyter notebook?</h3>
<p>This exercise is again a Jupyter notebook, rather than Pluto.  Why?  When using the GPU, it is possible to do things that require restarting Julia in order to recover from.  That requires shutting down the Pluto server.  The way that our Pluto servier is run from JupyterLab, that would require killing your whole JupyterLab session and requesting a new one.  This exercise has some custom CUDA kernels.  When I tested it, the notebook worked correctly.  But I encourage students to try tweaking things, and that could easily lead to some errors that require a restart.  So I decided to put this into a Jupyter notebook.  In Jupyter, you can go to the <em>Kernel</em> menu and select <em>Restart Kernel</em> to get a new kernel and try again.</p>
<h3>Setting up the GPU</h3>
<p>Remember that when using Jupyter, the order in which you execute cells affects the state of the notebook.  &#40;In contrast to in Pluto, where it figures out the order that cells are to be run, so we can put implementation details at the bottom of the notebook, where they&#39;re less distracting.&#41; Therefore, we place code in the order that it should be run.  We&#39;ll start loading the packages we&#39;ll use for writing our custom GPU kernels.</p>


<pre class='hljl'>
<span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>KernelAbstractions</span><span class='hljl-t'>   </span><span class='hljl-cs'># Allows writing code once for different brand GPUs</span><span class='hljl-t'>
</span><span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>CUDAKernels</span><span class='hljl-t'>    </span><span class='hljl-cs'># for NVIDIA GPUs</span><span class='hljl-t'>
</span><span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>BenchmarkTools</span><span class='hljl-t'>       </span><span class='hljl-cs'># for timing</span><span class='hljl-t'>
</span><span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>Markdown</span><span class='hljl-t'>             </span><span class='hljl-cs'># for responces</span>
</pre>



<p>As before we will check that the system we&#39;re running on has an NVIDIA GPU that we can use and check what&#39;s installed.</p>


<pre class='hljl'>
<span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>devices</span><span class='hljl-p'>()</span>
</pre>


<pre class="output">
CUDA.DeviceIterator&#40;&#41; for 1 devices:
0. Tesla P100-PCIE-12GB
</pre>



<pre class='hljl'>
<span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>versioninfo</span><span class='hljl-p'>()</span>
</pre>


<pre class="output">
CUDA toolkit 11.4, artifact installation
NVIDIA driver 440.64.0, for CUDA 10.2
CUDA driver 11.4

Libraries: 
- CUBLAS: 11.5.4
- CURAND: 10.2.5
- CUFFT: 10.5.1
- CUSOLVER: 11.2.0
- CUSPARSE: 11.6.0
- CUPTI: 14.0.0
- NVML: 10.0.0&#43;440.64.0
- CUDNN: 8.20.2 &#40;for CUDA 11.4.0&#41;
- CUTENSOR: 1.3.0 &#40;for CUDA 11.2.0&#41;

Toolchain:
- Julia: 1.6.0
- LLVM: 11.0.1
- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7
.0
- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_6
1, sm_62, sm_70, sm_72, sm_75, sm_80

1 device:
  0: Tesla P100-PCIE-12GB &#40;sm_60, 11.902 GiB / 11.912 GiB available&#41;
</pre>


<p>We&#39;ll make use of the GPU&#39;s warpsize later in the exericse, so let&#39;s get it now.</p>


<pre class='hljl'>
<span class='hljl-nf'>warpsize</span><span class='hljl-p'>(</span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>device</span><span class='hljl-p'>())</span>
</pre>


<pre class="output">
32
</pre>


<h2>CPU version of code to run in parallel</h2>
<p>We&#39;ll write two functions that we&#39;ll convert into GPU kernels.  The first, <code>calc_rv_circ</code>, will be a simple function, so we keep things simple.  The second, <code>calc_rv_kepler</code>, will call other several user-written functions.  We&#39;ll even place the code to solve Kepler&#39;s equation in a module to help with code maintainability.</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_circ</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>::</span><span class='hljl-n'>NamedTuple</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>P</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>.</span><span class='hljl-n'>P</span><span class='hljl-t'>
    </span><span class='hljl-n'>K</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>.</span><span class='hljl-n'>K</span><span class='hljl-t'>
    </span><span class='hljl-n'>M0</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>.</span><span class='hljl-n'>M0</span><span class='hljl-t'>
    </span><span class='hljl-n'>mean_anom</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>t</span><span class='hljl-oB'>*</span><span class='hljl-ni'>2</span><span class='hljl-n'>π</span><span class='hljl-oB'>/</span><span class='hljl-n'>P</span><span class='hljl-oB'>-</span><span class='hljl-n'>M0</span><span class='hljl-t'>
    </span><span class='hljl-n'>rv</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>K</span><span class='hljl-oB'>*</span><span class='hljl-nf'>sin</span><span class='hljl-p'>(</span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
calc_rv_circ &#40;generic function with 1 method&#41;
</pre>


<h3>Code for computing radial velocity of a Keplerian orbit</h3>


<pre class='hljl'>
<span class='hljl-s'>&quot;Solve Computing Eccentric Anomaly from Mean Anomally via Kepler&#39;s Equation&quot;</span><span class='hljl-t'>
</span><span class='hljl-k'>module</span><span class='hljl-t'> </span><span class='hljl-n'>KeplerEqn</span><span class='hljl-t'>
</span><span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>KernelAbstractions</span><span class='hljl-t'>  </span><span class='hljl-cs'># since we&#39;ll use KernelAbstractions to get slightly better performance.</span><span class='hljl-t'>
</span><span class='hljl-k'>export</span><span class='hljl-t'> </span><span class='hljl-n'>calc_ecc_anom</span><span class='hljl-t'>

</span><span class='hljl-s'>&quot;&quot;&quot;   ecc_anom_init_guess_danby(M, ecc)
Initial guess for eccentric anomaly given mean anomaly (M) and eccentricity (ecc)
    Based on &quot;The Solution of Kepler&#39;s Equations - Part Three&quot;  
    Danby, J. M. A. (1987) Journal: Celestial Mechanics, Volume 40, Issue 3-4, pp. 303-312  1987C
eMec..40..303D
&quot;&quot;&quot;</span><span class='hljl-t'>
</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>ecc_anom_init_guess_danby</span><span class='hljl-p'>(</span><span class='hljl-n'>M</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>k</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>convert</span><span class='hljl-p'>(</span><span class='hljl-nf'>typeof</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc</span><span class='hljl-p'>),</span><span class='hljl-nfB'>0.85</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>if</span><span class='hljl-p'>(</span><span class='hljl-n'>M</span><span class='hljl-oB'>&lt;</span><span class='hljl-nf'>zero</span><span class='hljl-p'>(</span><span class='hljl-n'>M</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-n'>M</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-n'>pi</span><span class='hljl-t'> </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-p'>(</span><span class='hljl-n'>M</span><span class='hljl-oB'>&lt;</span><span class='hljl-n'>pi</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>M</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>k</span><span class='hljl-oB'>*</span><span class='hljl-n'>ecc</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-n'>M</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>k</span><span class='hljl-oB'>*</span><span class='hljl-n'>ecc</span><span class='hljl-p'>;</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-s'>&quot;&quot;&quot;   update_ecc_anom_laguerre(E, M, ecc)
Update the current guess (E) for the solution to Kepler&#39;s equation given mean anomaly (M) and ecc
entricity (ecc)
   Based on &quot;An Improved Algorithm due to Laguerre for the Solution of Kepler&#39;s Equation&quot;
   Conway, B. A.  (1986) Celestial Mechanics, Volume 39, Issue 2, pp.199-211  1986CeMec..39..199C
&quot;&quot;&quot;</span><span class='hljl-t'>
</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>update_ecc_anom_laguerre</span><span class='hljl-p'>(</span><span class='hljl-n'>E</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>M</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-p'>)</span><span class='hljl-t'>
  </span><span class='hljl-n'>es</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ec</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-oB'>.*</span><span class='hljl-nf'>sincos</span><span class='hljl-p'>(</span><span class='hljl-n'>E</span><span class='hljl-p'>)</span><span class='hljl-t'>
  </span><span class='hljl-n'>F</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>E</span><span class='hljl-oB'>-</span><span class='hljl-n'>es</span><span class='hljl-p'>)</span><span class='hljl-oB'>-</span><span class='hljl-n'>M</span><span class='hljl-t'>
  </span><span class='hljl-n'>Fp</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>one</span><span class='hljl-p'>(</span><span class='hljl-n'>E</span><span class='hljl-p'>)</span><span class='hljl-oB'>-</span><span class='hljl-n'>ec</span><span class='hljl-t'>
  </span><span class='hljl-n'>Fpp</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>es</span><span class='hljl-t'>
  </span><span class='hljl-n'>n</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>5</span><span class='hljl-t'>
  </span><span class='hljl-n'>root</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>sqrt</span><span class='hljl-p'>(</span><span class='hljl-nf'>abs</span><span class='hljl-p'>((</span><span class='hljl-n'>n</span><span class='hljl-oB'>-</span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-oB'>*</span><span class='hljl-p'>((</span><span class='hljl-n'>n</span><span class='hljl-oB'>-</span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-oB'>*</span><span class='hljl-n'>Fp</span><span class='hljl-oB'>*</span><span class='hljl-n'>Fp</span><span class='hljl-oB'>-</span><span class='hljl-n'>n</span><span class='hljl-oB'>*</span><span class='hljl-n'>F</span><span class='hljl-oB'>*</span><span class='hljl-n'>Fpp</span><span class='hljl-p'>)))</span><span class='hljl-t'>
  </span><span class='hljl-n'>denom</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>Fp</span><span class='hljl-oB'>&gt;</span><span class='hljl-nf'>zero</span><span class='hljl-p'>(</span><span class='hljl-n'>E</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>Fp</span><span class='hljl-oB'>+</span><span class='hljl-n'>root</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-n'>Fp</span><span class='hljl-oB'>-</span><span class='hljl-n'>root</span><span class='hljl-t'>
  </span><span class='hljl-k'>return</span><span class='hljl-t'> </span><span class='hljl-n'>E</span><span class='hljl-oB'>-</span><span class='hljl-n'>n</span><span class='hljl-oB'>*</span><span class='hljl-n'>F</span><span class='hljl-oB'>/</span><span class='hljl-n'>denom</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-kd'>const</span><span class='hljl-t'> </span><span class='hljl-n'>default_ecc_anom_tol</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>1e-8</span><span class='hljl-t'>
</span><span class='hljl-s'>&quot;Loop to update the current estimate of the solution to Kepler&#39;s equation.&quot;</span><span class='hljl-t'>
</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_ecc_anom</span><span class='hljl-p'>(</span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>tol</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>default_ecc_anom_tol</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>default_max_its_laguerre</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>200</span><span class='hljl-t'>
    </span><span class='hljl-n'>max_its</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>default_max_its_laguerre</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nf'>zero</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;</span><span class='hljl-t'> </span><span class='hljl-nf'>one</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-n'>tol</span><span class='hljl-oB'>*</span><span class='hljl-ni'>100</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-nf'>one</span><span class='hljl-p'>(</span><span class='hljl-n'>tol</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-cs'>#M = mod(mean_anom,convert(typeof(mean_anom),2pi))</span><span class='hljl-t'>
    </span><span class='hljl-n'>M</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>rem2pi</span><span class='hljl-p'>(</span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>,</span><span class='hljl-n'>RoundNearest</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>E</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>ecc_anom_init_guess_danby</span><span class='hljl-p'>(</span><span class='hljl-n'>M</span><span class='hljl-p'>,</span><span class='hljl-n'>ecc</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-cs'># Slight optimization from our original CPU code here, to tell the GPU to try to unroll the first several itterations of the for loop. </span><span class='hljl-t'>
    </span><span class='hljl-cs'>#for i in 1:max_its</span><span class='hljl-t'>
    </span><span class='hljl-n'>KernelAbstractions</span><span class='hljl-oB'>.</span><span class='hljl-n'>Extras</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@unroll</span><span class='hljl-t'> </span><span class='hljl-ni'>6</span><span class='hljl-t'> </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-n'>max_its</span><span class='hljl-t'>
       </span><span class='hljl-n'>E_old</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>E</span><span class='hljl-t'>
       </span><span class='hljl-n'>E</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>update_ecc_anom_laguerre</span><span class='hljl-p'>(</span><span class='hljl-n'>E_old</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>M</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-p'>)</span><span class='hljl-t'>
       </span><span class='hljl-k'>if</span><span class='hljl-t'> </span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>E</span><span class='hljl-oB'>-</span><span class='hljl-n'>E_old</span><span class='hljl-p'>)</span><span class='hljl-oB'>&lt;</span><span class='hljl-nf'>convert</span><span class='hljl-p'>(</span><span class='hljl-nf'>typeof</span><span class='hljl-p'>(</span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>),</span><span class='hljl-n'>tol</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-k'>break</span><span class='hljl-t'> </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-k'>return</span><span class='hljl-t'> </span><span class='hljl-n'>E</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>end</span><span class='hljl-t'> </span><span class='hljl-cs'># module KeplerEqn</span>
</pre>


<pre class="output">
Main.##WeaveSandBox#257.KeplerEqn
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_true_anom</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>e</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>true_anom</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-oB'>*</span><span class='hljl-nf'>atan</span><span class='hljl-p'>(</span><span class='hljl-nf'>sqrt</span><span class='hljl-p'>((</span><span class='hljl-ni'>1</span><span class='hljl-oB'>+</span><span class='hljl-n'>e</span><span class='hljl-p'>)</span><span class='hljl-oB'>/</span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>-</span><span class='hljl-n'>e</span><span class='hljl-p'>))</span><span class='hljl-oB'>*</span><span class='hljl-nf'>tan</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-oB'>/</span><span class='hljl-ni'>2</span><span class='hljl-p'>))</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
calc_true_anom &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>::</span><span class='hljl-n'>NamedTuple</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>P</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>.</span><span class='hljl-n'>P</span><span class='hljl-t'>
    </span><span class='hljl-n'>K</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>.</span><span class='hljl-n'>K</span><span class='hljl-t'>
    </span><span class='hljl-n'>ecc</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>.</span><span class='hljl-n'>e</span><span class='hljl-t'>
    </span><span class='hljl-n'>ω</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>.</span><span class='hljl-n'>ω</span><span class='hljl-t'>
    </span><span class='hljl-n'>M0</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>.</span><span class='hljl-n'>M0</span><span class='hljl-t'>
    </span><span class='hljl-n'>mean_anom</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>t</span><span class='hljl-oB'>*</span><span class='hljl-ni'>2</span><span class='hljl-n'>π</span><span class='hljl-oB'>/</span><span class='hljl-n'>P</span><span class='hljl-oB'>-</span><span class='hljl-n'>M0</span><span class='hljl-t'>
    </span><span class='hljl-n'>ecc_anom</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>KeplerEqn</span><span class='hljl-oB'>.</span><span class='hljl-nf'>calc_ecc_anom</span><span class='hljl-p'>(</span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>,</span><span class='hljl-n'>ecc</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>true_anom</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_true_anom</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-p'>,</span><span class='hljl-n'>ecc</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>rv</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>cos</span><span class='hljl-p'>(</span><span class='hljl-n'>ω</span><span class='hljl-oB'>+</span><span class='hljl-n'>true_anom</span><span class='hljl-p'>)</span><span class='hljl-oB'>+</span><span class='hljl-n'>ecc</span><span class='hljl-oB'>*</span><span class='hljl-nf'>cos</span><span class='hljl-p'>(</span><span class='hljl-n'>ω</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>K</span><span class='hljl-oB'>/</span><span class='hljl-nf'>sqrt</span><span class='hljl-p'>((</span><span class='hljl-ni'>1</span><span class='hljl-oB'>-</span><span class='hljl-n'>ecc</span><span class='hljl-p'>)</span><span class='hljl-oB'>*</span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>+</span><span class='hljl-n'>ecc</span><span class='hljl-p'>))</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
calc_rv_kepler &#40;generic function with 1 method&#41;
</pre>


<h2>Writing GPU kernels with KernelAbstractions.jl</h2>
<p>The CPU version of <code>calc_rv_circ_kernel</code> and <code>calc_rv_kepler_kernel</code> operate on scalars.  For GPU calculations to be efficient, we need them to operate on arrays.  For some simple use cases &#40;like the examples below&#41;, we could just use <code>map</code>.  However, it is useful to start simple when demonstrating how to write a custom GPU kernel.  Below we&#39;ll using the <code>KernelAbstractions</code> <code>@kernel</code> macro to simplify writing our GPU kernel.  For example, rather than calculating the index to operate on explicitly from the thread and block indices, we&#39;ll use the <code>@index</code> macro to get the index for each thread to operate on.  There&#39;s also a useful <code>@Const</code> macro that allows us to specify that the data in our input arrays will remain constant, allowing for memory-related optimizations that aren&#39;t always possible. It will be up to us when we call the kernel to make sure that there as many threads as elements of <code>y</code> and <code>times</code>.  </p>


<pre class='hljl'>
<span class='hljl-n'>KernelAbstractions</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@kernel</span><span class='hljl-t'> </span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_circ_kernel</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nd'>@Const</span><span class='hljl-p'>(</span><span class='hljl-n'>times</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nd'>@Const</span><span class='hljl-p'>(</span><span class='hljl-n'>param</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>I</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nd'>@index</span><span class='hljl-p'>(</span><span class='hljl-n'>Global</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>t</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>times</span><span class='hljl-p'>[</span><span class='hljl-n'>I</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-n'>y</span><span class='hljl-p'>[</span><span class='hljl-n'>I</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_circ</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>=</span><span class='hljl-n'>param</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
calc_rv_circ_kernel &#40;generic function with 5 methods&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>KernelAbstractions</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@kernel</span><span class='hljl-t'> </span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_kepler_kernel</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nd'>@Const</span><span class='hljl-p'>(</span><span class='hljl-n'>times</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nd'>@Const</span><span class='hljl-p'>(</span><span class='hljl-n'>param</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>I</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nd'>@index</span><span class='hljl-p'>(</span><span class='hljl-n'>Global</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>t</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>times</span><span class='hljl-p'>[</span><span class='hljl-n'>I</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-n'>y</span><span class='hljl-p'>[</span><span class='hljl-n'>I</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-n'>param</span><span class='hljl-oB'>=</span><span class='hljl-n'>param</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
calc_rv_kepler_kernel &#40;generic function with 5 methods&#41;
</pre>


<p>One great feature of the <code>KernelAbstractions.jl</code> package is that we can the core code for kernels once, and turn them into kernels that can run on a CPU, an NVIDIA GPU or an AMD GPU.  In order to get a kernel that can be executed, we call the function returned by the <code>@kernel</code> macro, specifying the hardware we will use to execute the kernel.   Optionally, we can specify the <em>workgroup size</em> and the size of the global indices to be used, when we request the kernel.  If we don&#39;t provide that info now, then we&#39;ll need to provide it at runtime. As we discussed previously, its often useful for the compiler to have information at compile time.  Therefore, we&#39;ll specify the workgroup size at compile time in building our kernels below.  We&#39;ll leave the size of the global indices as a dynamic parameter, so that we don&#39;t need to recompile a kernel for each problem size.</p>


<pre class='hljl'>
<span class='hljl-n'>cpu_kernel_calc_rv_circ!</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_circ_kernel</span><span class='hljl-p'>(</span><span class='hljl-nf'>CPU</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-ni'>16</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
KernelAbstractions.Kernel&#123;KernelAbstractions.CPU, KernelAbstractions.NDIter
ation.StaticSize&#123;&#40;16,&#41;&#125;, KernelAbstractions.NDIteration.DynamicSize, typeof
&#40;Main.##WeaveSandBox#257.cpu_calc_rv_circ_kernel&#41;&#125;&#40;Main.##WeaveSandBox#257.
cpu_calc_rv_circ_kernel&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>cpu_kernel_calc_rv_kepler!</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_kepler_kernel</span><span class='hljl-p'>(</span><span class='hljl-nf'>CPU</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-ni'>16</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
KernelAbstractions.Kernel&#123;KernelAbstractions.CPU, KernelAbstractions.NDIter
ation.StaticSize&#123;&#40;16,&#41;&#125;, KernelAbstractions.NDIteration.DynamicSize, typeof
&#40;Main.##WeaveSandBox#257.cpu_calc_rv_kepler_kernel&#41;&#125;&#40;Main.##WeaveSandBox#25
7.cpu_calc_rv_kepler_kernel&#41;
</pre>


<h2>Generate data for testing kernels</h2>
<p>Let&#39;s generate some simulated data for testing our CPU kernels.  Below are functions that will make it easy to generate datasets of various sizes later in the exercise.</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>generate_obs_data</span><span class='hljl-p'>(</span><span class='hljl-n'>times</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>σ_obs</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>model</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>num_obs</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>times</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>rv_true</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>model</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-n'>times</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>rv_obs</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>rv_true</span><span class='hljl-t'> </span><span class='hljl-oB'>.+</span><span class='hljl-t'> </span><span class='hljl-n'>σ_obs</span><span class='hljl-oB'>.*</span><span class='hljl-nf'>randn</span><span class='hljl-p'>(</span><span class='hljl-n'>num_obs</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>obs_data</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(;</span><span class='hljl-t'> </span><span class='hljl-n'>t</span><span class='hljl-oB'>=</span><span class='hljl-n'>times</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>rv</span><span class='hljl-oB'>=</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>σ</span><span class='hljl-oB'>=</span><span class='hljl-n'>σ_obs</span><span class='hljl-oB'>.*</span><span class='hljl-nf'>ones</span><span class='hljl-p'>(</span><span class='hljl-n'>num_obs</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>generate_obs_data</span><span class='hljl-p'>(;</span><span class='hljl-n'>time_span</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>num_obs</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>σ_obs</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>model</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>days_in_year</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>365.2425</span><span class='hljl-t'>
    </span><span class='hljl-n'>times</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>sort</span><span class='hljl-p'>(</span><span class='hljl-n'>time_span</span><span class='hljl-oB'>*</span><span class='hljl-n'>days_in_year</span><span class='hljl-oB'>*</span><span class='hljl-nf'>rand</span><span class='hljl-p'>(</span><span class='hljl-n'>num_obs</span><span class='hljl-p'>))</span><span class='hljl-t'>
    </span><span class='hljl-nf'>generate_obs_data</span><span class='hljl-p'>(</span><span class='hljl-n'>times</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>=</span><span class='hljl-n'>param</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>σ_obs</span><span class='hljl-oB'>=</span><span class='hljl-n'>σ_obs</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>model</span><span class='hljl-oB'>=</span><span class='hljl-n'>model</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
generate_obs_data &#40;generic function with 2 methods&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>begin</span><span class='hljl-t'>
    </span><span class='hljl-n'>P_true</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>3.0</span><span class='hljl-t'>
    </span><span class='hljl-n'>K_true</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>5.0</span><span class='hljl-t'>
    </span><span class='hljl-n'>e_true</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.4</span><span class='hljl-t'>
    </span><span class='hljl-n'>ω_true</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>π</span><span class='hljl-oB'>/</span><span class='hljl-ni'>4</span><span class='hljl-t'>
    </span><span class='hljl-n'>M0_true</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>π</span><span class='hljl-oB'>/</span><span class='hljl-ni'>4</span><span class='hljl-t'>
    </span><span class='hljl-n'>θ_true</span><span class='hljl-t'>  </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(;</span><span class='hljl-n'>P</span><span class='hljl-oB'>=</span><span class='hljl-n'>P_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>K</span><span class='hljl-oB'>=</span><span class='hljl-n'>K_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>e</span><span class='hljl-oB'>=</span><span class='hljl-n'>e_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ω</span><span class='hljl-oB'>=</span><span class='hljl-n'>ω_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>M0</span><span class='hljl-oB'>=</span><span class='hljl-n'>M0_true</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
&#40;P &#61; 3.0, K &#61; 5.0, e &#61; 0.4, ω &#61; 0.7853981633974483, M0 &#61; 0.7853981633974483
&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>n_obs</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>1_000_000</span><span class='hljl-t'>
</span><span class='hljl-n'>time_span_in_years</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-t'>
</span><span class='hljl-n'>obs_data</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>generate_obs_data</span><span class='hljl-p'>(</span><span class='hljl-n'>time_span</span><span class='hljl-oB'>=</span><span class='hljl-n'>time_span_in_years</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>num_obs</span><span class='hljl-oB'>=</span><span class='hljl-n'>n_obs</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>=</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>σ_obs</span><span class='hljl-oB'>=</span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>model</span><span class='hljl-oB'>=</span><span class='hljl-n'>calc_rv_kepler</span><span class='hljl-p'>);</span>
</pre>



<p>It&#39;s often nice to visualize our data, just to make sure our code is doing what we expect.  I&#39;ve commented out the plotting code, so the pbs script will work.  Also, note that if we plotted all the data, it would take a <em>very</em> long time, you&#39;ll be better off just ploting a random sample of the points.</p>


<pre class='hljl'>
<span class='hljl-cs'>#using Plots</span>
</pre>




<pre class='hljl'>
<span class='hljl-cm'>#=
idx_plt = length(obs_data.t) &lt;= 100 ? (1:length(obs_data.t))  : rand(1:length(obs_data.t),100) 
plt = scatter(obs_data.t[idx_plt],obs_data.rv[idx_plt],yerr=obs_data.σ[idx_plt],legend=:none,ms=2)
xlabel!(plt,&quot;Time (d)&quot;)
ylabel!(plt,&quot;RV (m/s)&quot;)
title!(plt,&quot;Simulated Data&quot;)
=#</span>
</pre>



<h2>Executing a kernel</h2>
<p>First, we&#39;ll make sure that the CPU kernel we generated with KernelAbstractions gives similar results.  Since our kernel needs an array to write it&#39;s output to, we&#39;ll allocate memory for that.  Then we&#39;ll pass the optional arguement <code>ndrange</code> to tell it the size of the global indices that it should use for the times and outputs.  Kernel calls can be asynchronous, so they return an <em>event</em> that can be used to check whether the kernel has completed.  We call <code>wait</code> on the event to make sure our kernel has completed its work before using the results.</p>


<pre class='hljl'>
<span class='hljl-n'>output_cpu</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>cpu_event</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>cpu_kernel_calc_rv_circ!</span><span class='hljl-p'>(</span><span class='hljl-n'>output_cpu</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>))</span><span class='hljl-t'>
</span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-n'>cpu_event</span><span class='hljl-p'>)</span>
</pre>



<p>Now we&#39;ll try performing the same calculation on the GPU.  We generate a GPU kernel for NVIDIA GPUs by passing <code>CUDADevice&#40;&#41;</code> instead of <code>CPU&#40;&#41;</code>.  We&#39;ll specify a workgroup size equal to the warpsize on our GPUs.  </p>


<pre class='hljl'>
<span class='hljl-n'>output_gpu_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>zeros</span><span class='hljl-p'>(</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>));</span><span class='hljl-t'>
</span><span class='hljl-n'>obs_times_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>cu</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>);</span>
</pre>




<pre class='hljl'>
<span class='hljl-n'>gpu_kernel_calc_rv_circ!</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_circ_kernel</span><span class='hljl-p'>(</span><span class='hljl-nf'>CUDADevice</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-ni'>32</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
KernelAbstractions.Kernel&#123;CUDAKernels.CUDADevice, KernelAbstractions.NDIter
ation.StaticSize&#123;&#40;32,&#41;&#125;, KernelAbstractions.NDIteration.DynamicSize, typeof
&#40;Main.##WeaveSandBox#257.gpu_calc_rv_circ_kernel&#41;&#125;&#40;Main.##WeaveSandBox#257.
gpu_calc_rv_circ_kernel&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>gpu_event</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_kernel_calc_rv_circ!</span><span class='hljl-p'>(</span><span class='hljl-n'>output_gpu_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>))</span><span class='hljl-t'>
</span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-n'>gpu_event</span><span class='hljl-p'>)</span>
</pre>



<h2>Accuracy of GPU calculations</h2>
<p>Before we start benchmarking, let&#39;s check the the results from the GPU kernel are accurate.    </p>


<pre class='hljl'>
<span class='hljl-nf'>maximum</span><span class='hljl-p'>(</span><span class='hljl-n'>abs</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-n'>output_gpu_d</span><span class='hljl-p'>)</span><span class='hljl-oB'>.-</span><span class='hljl-n'>output_cpu</span><span class='hljl-p'>))</span>
</pre>


<pre class="output">
0.0001597069740781748
</pre>


<p>That&#39;s likely larger than we&#39;d normally expect.  What could have caused it.  It&#39;s good to check the types of the arrays on the device.</p>


<pre class='hljl'>
<span class='hljl-nf'>typeof</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nf'>typeof</span><span class='hljl-p'>(</span><span class='hljl-n'>output_gpu_d</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
&#40;CUDA.CuArray&#123;Float32, 1, CUDA.Mem.DeviceBuffer&#125;, CUDA.CuArray&#123;Float32, 1, 
CUDA.Mem.DeviceBuffer&#125;&#41;
</pre>


<p>Why?  Look back at where we allocated <code>obs_times_d</code> and <code>output_gpu_d</code>.  <code>cu</code> defaults to sending arrays as Float32&#39;s rather than Float64&#39;s.  We can be explicit about what type we want.  Additionally, the default element type for <code>CUDA.zeros</code> is Float32, rather than Float64 &#40;like it is for <code>Base.zeros</code>&#41;.  </p>


<pre class='hljl'>
<span class='hljl-n'>obs_times_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>convert</span><span class='hljl-p'>(</span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>{</span><span class='hljl-n'>Float64</span><span class='hljl-p'>,</span><span class='hljl-ni'>1</span><span class='hljl-p'>},</span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>);</span><span class='hljl-t'>
</span><span class='hljl-n'>output_gpu_d64</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>zeros</span><span class='hljl-p'>(</span><span class='hljl-n'>Float64</span><span class='hljl-p'>,</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>));</span><span class='hljl-t'>
</span><span class='hljl-n'>gpu_event</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_kernel_calc_rv_circ!</span><span class='hljl-p'>(</span><span class='hljl-n'>output_gpu_d64</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>))</span><span class='hljl-t'>
</span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-n'>gpu_event</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>maximum</span><span class='hljl-p'>(</span><span class='hljl-n'>abs</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-n'>output_gpu_d64</span><span class='hljl-p'>)</span><span class='hljl-oB'>.-</span><span class='hljl-n'>output_cpu</span><span class='hljl-p'>))</span>
</pre>


<pre class="output">
8.881784197001252e-16
</pre>


<p>That should be much better.  &#40;It&#39;s still not zero, since the GPU defaults to allowing the optimizer to perform optimizations that are not IEEE-compliant.&#41; </p>

<h2>Comparing CPU &amp; GPU Performance</h2>
<p>Next, we&#39;ll comapre the time to execute the CPU and GPU kernels.  In the test below, we&#39;ll compute the predicted radial velcoity for the first <code>num_obs_to_eval</code> time.  The default code below is for the full set of observation times.  But you may wish to try comparing the performance for fewer observations &#40;without having to regenerate the simulated data&#41;.</p>


<pre class='hljl'>
<span class='hljl-n'>num_obs_to_eval</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
1000000
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'>  </span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-nf'>cpu_kernel_calc_rv_circ!</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>output_cpu</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=$</span><span class='hljl-n'>num_obs_to_eval</span><span class='hljl-t'> </span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 230 samples with 1 evaluation.
 Range &#40;min … max&#41;:  4.303 ms …   6.186 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 0.00&#37;
 Time  &#40;median&#41;:     4.347 ms               ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   4.357 ms ± 129.420 μs  ┊ GC &#40;mean ± σ&#41;:  0.00&#37; ± 0.00&#37;

   █▅▄        ▁ ▅                                              
  ▇███▇▃▂▃▃▁▃▄█▇█▂▃▃▂▃▁▃▄▅▇▄▁▂▂▃▂▁▂▃▂▃▃▁▁▂▁▁▁▁▂▁▁▂▁▂▁▂▁▂▁▁▁▁▂ ▃
  4.3 ms          Histogram: frequency by time        4.52 ms &lt;

 Memory estimate: 3.98 KiB, allocs estimate: 67.
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-nf'>gpu_kernel_calc_rv_circ!</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>output_gpu_d64</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=$</span><span class='hljl-n'>num_obs_to_eval</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 6505 samples with 1 evaluation.
 Range &#40;min … max&#41;:  140.933 μs … 407.929 μs  ┊ GC &#40;min … max&#41;: 0.00&#37; … 0.0
0&#37;
 Time  &#40;median&#41;:     143.594 μs               ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   148.089 μs ±  12.161 μs  ┊ GC &#40;mean ± σ&#41;:  0.00&#37; ± 0.0
0&#37;

   ▇█▆▃▃▃▄▃▁▁           ▁             ▁                         ▁
  ██████████████▇▆▆█▆▆▆██▇▆▆▆▇█▇▇▇▆▆▅▇██▆▆▆▄▅▅▅▄▄▄▂▂▄▂▃▄▃█▇▆▄▂▆ █
  141 μs        Histogram: log&#40;frequency&#41; by time        202 μs &lt;

 Memory estimate: 3.77 KiB, allocs estimate: 77.
</pre>


<p>2a.  How much faster was performing the computations on the GPU than on the CPU? </p>


<pre class='hljl'>
<span class='hljl-n'>response_2a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>


<h2>Performance as a function of workgroup size</h2>
<p>When we generate a kernel to run on the GPU, the workgroup size can make a significant difference in the performance.  My understanding is that the workgroup size is equivalent to the block size on NVIDIA GPUs.  response_2a &#61; missing # md&quot;Insert response&quot;</p>
<p>2b.  What do you predict for the compute time for the same GPU kernel, except with the workgroup size set to 1?</p>


<pre class='hljl'>
<span class='hljl-n'>response_2b</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>



<pre class='hljl'>
<span class='hljl-n'>gpu_kernel_calc_rv_circ_alt!</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_circ_kernel</span><span class='hljl-p'>(</span><span class='hljl-nf'>CUDADevice</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
KernelAbstractions.Kernel&#123;CUDAKernels.CUDADevice, KernelAbstractions.NDIter
ation.StaticSize&#123;&#40;1,&#41;&#125;, KernelAbstractions.NDIteration.DynamicSize, typeof&#40;
Main.##WeaveSandBox#257.gpu_calc_rv_circ_kernel&#41;&#125;&#40;Main.##WeaveSandBox#257.g
pu_calc_rv_circ_kernel&#41;
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-nf'>gpu_kernel_calc_rv_circ_alt!</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>output_gpu_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=$</span><span class='hljl-n'>num_obs_to_eval</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 57 samples with 1 evaluation.
 Range &#40;min … max&#41;:   2.760 ms … 100.320 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 0.00
&#37;
 Time  &#40;median&#41;:      5.255 ms               ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   19.026 ms ±  26.802 ms  ┊ GC &#40;mean ± σ&#41;:  0.00&#37; ± 0.00
&#37;

  █                                                             
  █▅▇▅▅█▅▅▇▁▁█▁▅▇▁▅▁▁▅▁▅▁▁▁▁▅▁▁▁▁▁▁▅▁▁▁▁▁▁▅▁▁▁▁▅▅▁▁▁▁▁▁▁▁▁▁▁▁▇ ▁
  2.76 ms       Histogram: log&#40;frequency&#41; by time       100 ms &lt;

 Memory estimate: 3.77 KiB, allocs estimate: 77.
</pre>


<p>2c.  How did the performance with a workgrop size of 1 compare to your predictions?  </p>


<pre class='hljl'>
<span class='hljl-n'>response_2c</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>


<p>If you&#39;re likely to be using a GPU for parallelizing your project code, then try adjusting the workgroup size to other values &#40;between 1 and twice the warpsize&#41; and see how it affects the performance.</p>

<h2>Comparing performance of kernels</h2>
<p>So far, we&#39;ve been benchmarking a relatively simple kernel &#40;some arithmetic and one trig function&#41;.  Now, we&#39;ll try switching to computing the radial velocity assuming a Keplerian orbit.  First, we&#39;ll use a CPU version of the kernel.  </p>


<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-nf'>cpu_kernel_calc_rv_kepler!</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>output_cpu</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>)))</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 15 samples with 1 evaluation.
 Range &#40;min … max&#41;:  67.117 ms …  67.971 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 0.00
&#37;
 Time  &#40;median&#41;:     67.293 ms               ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   67.423 ms ± 257.460 μs  ┊ GC &#40;mean ± σ&#41;:  0.00&#37; ± 0.00
&#37;

  █    █ ██████        █    ██   █ █                       █ █  
  █▁▁▁▁█▁██████▁▁▁▁▁▁▁▁█▁▁▁▁██▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁█ ▁
  67.1 ms         Histogram: frequency by time           68 ms &lt;

 Memory estimate: 3.97 KiB, allocs estimate: 66.
</pre>


<p>Next, we&#39;ll create a GPU kernel to benchmark the Keplerian calculation on the GPU.</p>


<pre class='hljl'>
<span class='hljl-n'>gpu_kernel_calc_rv_kepler!</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_kepler_kernel</span><span class='hljl-p'>(</span><span class='hljl-n'>CUDAKernels</span><span class='hljl-oB'>.</span><span class='hljl-nf'>CUDADevice</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-ni'>32</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
KernelAbstractions.Kernel&#123;CUDAKernels.CUDADevice, KernelAbstractions.NDIter
ation.StaticSize&#123;&#40;32,&#41;&#125;, KernelAbstractions.NDIteration.DynamicSize, typeof
&#40;Main.##WeaveSandBox#257.gpu_calc_rv_kepler_kernel&#41;&#125;&#40;Main.##WeaveSandBox#25
7.gpu_calc_rv_kepler_kernel&#41;
</pre>


<p>2d.   Before you run the benchmarks, what do you expect for the GPU performance if we use a workgroup size equal to the warpsize?  What if we use a workgroup size of 1?  Explain your reasoning.  </p>


<pre class='hljl'>
<span class='hljl-n'>response_2d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-nf'>gpu_kernel_calc_rv_kepler!</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>output_gpu_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>)))</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 1570 samples with 1 evaluation.
 Range &#40;min … max&#41;:  491.665 μs … 783.279 μs  ┊ GC &#40;min … max&#41;: 0.00&#37; … 0.0
0&#37;
 Time  &#40;median&#41;:     615.361 μs               ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   622.487 μs ±  41.379 μs  ┊ GC &#40;mean ± σ&#41;:  0.00&#37; ± 0.0
0&#37;

                        ▆     █▁ ▁         ▁                     
  ▂▂▁▂▁▂▁▁▁▂▂▁▁▂▂▂▁▃▄▅▅▆██▆█▆▆██▆██▇▅▅▆▆██▅█▇▆▆▅▄▅▇▄▅▅▅▃▄▃▂▄▄▃▃ ▄
  492 μs           Histogram: frequency by time          717 μs &lt;

 Memory estimate: 3.77 KiB, allocs estimate: 77.
</pre>



<pre class='hljl'>
<span class='hljl-n'>gpu_kernel_calc_rv_kepler_alt!</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_kepler_kernel</span><span class='hljl-p'>(</span><span class='hljl-n'>CUDAKernels</span><span class='hljl-oB'>.</span><span class='hljl-nf'>CUDADevice</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-nf'>gpu_kernel_calc_rv_kepler_alt!</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>output_gpu_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>)))</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 89 samples with 1 evaluation.
 Range &#40;min … max&#41;:  11.278 ms … 11.511 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 0.00&#37;
 Time  &#40;median&#41;:     11.335 ms              ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   11.341 ms ± 45.088 μs  ┊ GC &#40;mean ± σ&#41;:  0.00&#37; ± 0.00&#37;

   █            ▂▂             ▂ ▆ ▄▂                          
  ▆█▆█▄█▆▆▆▆▆█▆▁██▁▄▆▄▁▄▁▁█▄█▁▆█▆█▄███▄▁▄▄▁▁▁▁▁█▁▁▁▄▁▁▁▁▁▁▁▁▄ ▁
  11.3 ms         Histogram: frequency by time        11.5 ms &lt;

 Memory estimate: 3.77 KiB, allocs estimate: 77.
</pre>


<p>2e.  How did the benchmarking results for the GPU version compare to your expectations?  What could explain the differences? </p>


<pre class='hljl'>
<span class='hljl-n'>response_2e</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>


<h2>High-level GPU Programming with FLoops</h2>
<p>In <a href="https://github.com/PsuAstro528/lab6-start">Lab 6</a>, we saw how we could write parallel for either serial, multithreaded or distributed architectures using <a href="https://juliafolds.github.io/FLoops.jl/dev/">FLoops.jl</a>.   The <a href="https://github.com/JuliaFolds/FoldsCUDA.jl">FoldsCUDA.jl</a> package provided an executor that allows FLoops to compile code for the GPU.  THe usual limitations about GPU kernels not being able to allocate memory still apply.  In order for the GPU kernel to be allowed to write to GPU arrays, we use the <code>referencable&#40;&#41;</code> function and an unusual syntax shown below.  </p>


<pre class='hljl'>
<span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>FLoops</span><span class='hljl-t'>
</span><span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>FoldsCUDA</span><span class='hljl-t'>
</span><span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>Referenceables</span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-n'>referenceable</span>
</pre>




<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_kepler_floops</span><span class='hljl-p'>(</span><span class='hljl-n'>output</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>times</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>ex</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'>  </span><span class='hljl-nf'>ThreadedEx</span><span class='hljl-p'>()</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
     </span><span class='hljl-nd'>@floop</span><span class='hljl-t'> </span><span class='hljl-n'>ex</span><span class='hljl-t'> </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>t_i</span><span class='hljl-p'>,</span><span class='hljl-n'>rv_i</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-nf'>zip</span><span class='hljl-p'>(</span><span class='hljl-nf'>referenceable</span><span class='hljl-p'>(</span><span class='hljl-n'>times</span><span class='hljl-p'>),</span><span class='hljl-nf'>referenceable</span><span class='hljl-p'>(</span><span class='hljl-n'>output</span><span class='hljl-p'>))</span><span class='hljl-t'>
        </span><span class='hljl-n'>rv_i</span><span class='hljl-p'>[]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>t_i</span><span class='hljl-p'>[],</span><span class='hljl-n'>param</span><span class='hljl-oB'>=</span><span class='hljl-n'>param</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-n'>output</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
calc_rv_kepler_floops &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>output_cpu_floops</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>calc_rv_kepler_floops</span><span class='hljl-p'>(</span><span class='hljl-n'>output_cpu_floops</span><span class='hljl-p'>,</span><span class='hljl-n'>obs_data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>all</span><span class='hljl-p'>(</span><span class='hljl-n'>output_cpu_floops</span><span class='hljl-t'> </span><span class='hljl-oB'>.≈</span><span class='hljl-t'> </span><span class='hljl-n'>output_cpu</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
true
</pre>



<pre class='hljl'>
<span class='hljl-n'>output_gpu_floops</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>calc_rv_kepler_floops</span><span class='hljl-p'>(</span><span class='hljl-n'>output_gpu_floops</span><span class='hljl-p'>,</span><span class='hljl-n'>obs_times_d</span><span class='hljl-p'>,</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ex</span><span class='hljl-oB'>=</span><span class='hljl-nf'>CUDAEx</span><span class='hljl-p'>())</span><span class='hljl-t'>
</span><span class='hljl-nf'>all</span><span class='hljl-p'>(</span><span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-n'>output_gpu_floops</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.≈</span><span class='hljl-t'> </span><span class='hljl-n'>output_cpu</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
true
</pre>


<h2>Improving performance by performing reductions on GPU</h2>
<p>In the previous calculations, there was a substantial ammount of data to be transfered back from the GPU to CPU.  Often, we don&#39;t need all the data to be moved back to the CPU, since we&#39;re primarily interested in one or more summary statistics.  For example, we might be interested in the chi-squared statistic for comparing our model predictions to the data.  In that case, we&#39;ll only need to return a very small ammount of data from the GPU back to the CPU.  Below, we&#39;ll see how that affects the performance.  </p>
<p>First, let&#39;s try calculating chi-squared the obvious way on the CPU.</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_chisq_rv_circ_cpu_simple</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>rv</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-t'> 
    </span><span class='hljl-nf'>sum</span><span class='hljl-p'>(((</span><span class='hljl-n'>calc_rv_circ</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>=</span><span class='hljl-n'>param</span><span class='hljl-p'>)</span><span class='hljl-oB'>.-</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>rv</span><span class='hljl-p'>)</span><span class='hljl-oB'>./</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-oB'>.^</span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
calc_chisq_rv_circ_cpu_simple &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_chisq_rv_kepler_cpu_simple</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>rv</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-t'> 
    </span><span class='hljl-nf'>sum</span><span class='hljl-p'>(((</span><span class='hljl-n'>calc_rv_kepler</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>=</span><span class='hljl-n'>param</span><span class='hljl-p'>)</span><span class='hljl-oB'>.-</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>rv</span><span class='hljl-p'>)</span><span class='hljl-oB'>./</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-oB'>.^</span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
calc_chisq_rv_kepler_cpu_simple &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>chisq_from_cpu_simple</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_chisq_rv_kepler_cpu_simple</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_true</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
996913.1499679472
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_chisq_rv_kepler_cpu_simple</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_data</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 5 samples with 1 evaluation.
 Range &#40;min … max&#41;:  248.747 ms … 249.982 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 0.0
0&#37;
 Time  &#40;median&#41;:     248.821 ms               ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   249.215 ms ± 602.144 μs  ┊ GC &#40;mean ± σ&#41;:  0.00&#37; ± 0.0
0&#37;

  ██ █                                             █          █  
  ██▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁█ ▁
  249 ms           Histogram: frequency by time          250 ms &lt;

 Memory estimate: 7.63 MiB, allocs estimate: 2.
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>cpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>rv_obs</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>σ</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nf'>ismissing</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>||</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>rv_pred</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>ismissing</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'>
    </span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-nf'>cpu_kernel_calc_rv_kepler!</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_pred</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>)))</span><span class='hljl-t'>
    </span><span class='hljl-n'>χ²</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>sum</span><span class='hljl-p'>(((</span><span class='hljl-n'>rv_pred</span><span class='hljl-oB'>.-</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-oB'>./</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-oB'>.^</span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>cpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>::</span><span class='hljl-n'>NamedTuple</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nf'>cpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>rv</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>σ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
cpu_calc_chisq_kepler &#40;generic function with 2 methods&#41;
</pre>



<pre class='hljl'>
<span class='hljl-nf'>cpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_true</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>≈</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_chisq_rv_kepler_cpu_simple</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_true</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
true
</pre>


<p>Now let&#39;s try the same calculation on the CPU using the CPU kernel built by <code>KernelAbstractions</code>.</p>


<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-nf'>cpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_data</span><span class='hljl-p'>,</span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 15 samples with 1 evaluation.
 Range &#40;min … max&#41;:  69.853 ms … 76.344 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 4.00&#37;
 Time  &#40;median&#41;:     70.458 ms              ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   71.222 ms ±  1.951 ms  ┊ GC &#40;mean ± σ&#41;:  0.57&#37; ± 1.42&#37;

  ▁█ ███▁▁    ▁         ▁                         ▁         ▁  
  ██▁█████▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█ ▁
  69.9 ms         Histogram: frequency by time        76.3 ms &lt;

 Memory estimate: 15.26 MiB, allocs estimate: 71.
</pre>


<p>That was likely much faster than the simple way&#33;  How is that possible?  The <code>KernelAbstractions</code> package is also parallelizing the calculation over multiple threads &#40;assuming that you launched julia using multipel threads&#41;.   Let&#39;s check how many threads are avaliable.</p>


<pre class='hljl'>
<span class='hljl-n'>Threads</span><span class='hljl-oB'>.</span><span class='hljl-nf'>nthreads</span><span class='hljl-p'>()</span>
</pre>


<pre class="output">
4
</pre>


<p>Since KernelAbstractions is primarily geared towards GPU computing, it might not generate code that is as efficient as some other packages designed for multi-threaded computing.  Nevertheless, it likely does pretty well on a relatively simple example like this.</p>

<h3>Performing reduction using Array interface to GPU</h3>
<p>Since our GPU kernel writes its output to an array on the GPU, we can use the array interface to the GPU to perform the reduction.</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-oB'>::</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>rv_obs</span><span class='hljl-oB'>::</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>σ</span><span class='hljl-oB'>::</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-oB'>::</span><span class='hljl-nf'>Union</span><span class='hljl-p'>{</span><span class='hljl-n'>Missing</span><span class='hljl-p'>,</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>}</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nf'>ismissing</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>||</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>rv_pred</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>ismissing</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'>
    </span><span class='hljl-nf'>gpu_kernel_calc_rv_kepler!</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_pred</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>))</span><span class='hljl-t'>
    </span><span class='hljl-n'>χ²</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>sum</span><span class='hljl-p'>(((</span><span class='hljl-n'>rv_pred</span><span class='hljl-oB'>.-</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-oB'>./</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-oB'>.^</span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_calc_chisq_kepler &#40;generic function with 1 method&#41;
</pre>


<p>Note that, previously, we didn&#39;t transfer the observed velocity and uncertainty to the GPU.  In order to perform the reduction on the GPU, we&#39;ll want to do that.  To make it more convenient, we can write a wrapper function that accepts AbstractArrays and transfers them to the GPU if necessary.  We&#39;ll also provide an optional workspace parameter, so that we can pass a preallocated array to store the predicted radial velocities in.</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>rv_obs</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>σ</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>t_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>isa</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>t</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-nf'>convert</span><span class='hljl-p'>(</span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>{</span><span class='hljl-nf'>eltype</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>),</span><span class='hljl-ni'>1</span><span class='hljl-p'>},</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>rv_obs_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>isa</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>,</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>rv_obs</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-nf'>convert</span><span class='hljl-p'>(</span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>{</span><span class='hljl-nf'>eltype</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>),</span><span class='hljl-ni'>1</span><span class='hljl-p'>},</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>σ_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>isa</span><span class='hljl-p'>(</span><span class='hljl-n'>σ</span><span class='hljl-p'>,</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>σ</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-nf'>convert</span><span class='hljl-p'>(</span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>{</span><span class='hljl-nf'>eltype</span><span class='hljl-p'>(</span><span class='hljl-n'>σ</span><span class='hljl-p'>),</span><span class='hljl-ni'>1</span><span class='hljl-p'>},</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>workspace_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>isa</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>CuArray</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>t_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nf'>gpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>t_d</span><span class='hljl-p'>,</span><span class='hljl-n'>rv_obs_d</span><span class='hljl-p'>,</span><span class='hljl-n'>σ_d</span><span class='hljl-p'>,</span><span class='hljl-n'>θ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-oB'>=</span><span class='hljl-n'>workspace_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_calc_chisq_kepler &#40;generic function with 2 methods&#41;
</pre>


<p>Thinking back to our best-practices for scientific software development readings and discussion, passing several arrays to a function can be a bit dangerous, since we have to remember the correct order.  GPU kernels have some limitations, but we can compensate by wrapping our GPU calls with CPU-level functions that have a safer interface.  For example, we can make a nice wrapper function that takes a NamedTuple of arrays and a set of parameters, with an optional named parameter for a pre-allocated workspace.</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>::</span><span class='hljl-n'>NamedTuple</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nf'>gpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>rv</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>σ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_calc_chisq_kepler &#40;generic function with 3 methods&#41;
</pre>


<p>Before benchmarking, let&#39;s make sure that the results are acceptably accurate.</p>


<pre class='hljl'>
<span class='hljl-nf'>gpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_true</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>≈</span><span class='hljl-t'> </span><span class='hljl-nf'>cpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_true</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
true
</pre>


<p>2f.  What do you predict for the time required to transfer the input data to the GPU, to compute the predicted velocities &#40;allocating GPU memory to hold the results&#41;, to calcualte chi-squared on the GPU and to return just the chi-squared value to the CPU?</p>


<pre class='hljl'>
<span class='hljl-n'>response_2f</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_data</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 340 samples with 1 evaluation.
 Range &#40;min … max&#41;:  2.808 ms …  19.934 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 31.65
&#37;
 Time  &#40;median&#41;:     2.878 ms               ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   2.941 ms ± 931.288 μs  ┊ GC &#40;mean ± σ&#41;:  0.63&#37; ±  1.72
&#37;

  ▁█▆         ▅▂▄    ▂                                         
  ████▄▁▁▁▁▁▁▆███▇▄▄▇█▆▄▃▄▄▂▁▂▂▃▃▅█▆▅▃▃▄▃▁▂▁▁▁▁▁▃▁▁▁▁▁▂▁▁▁▁▁▂ ▃
  2.81 ms         Histogram: frequency by time         3.1 ms &lt;

 Memory estimate: 12.17 KiB, allocs estimate: 224.
</pre>


<p>2g.  How did the benchmarking results compare to your prediction?  What might explain any differences?</p>


<pre class='hljl'>
<span class='hljl-n'>response_2g</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>


<p>Now, we&#39;ll repeat the benchmarking, but using input data already loaded onto the GPU and a pre-alocated workspace on the GPU.  The function below helps with that.  Don&#39;t worry about the non-obvious syntax.</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>convert_namedtuple_of_arrays_to_namedtuple_of_cuarrays</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-oB'>::</span><span class='hljl-nf'>NamedTuple</span><span class='hljl-p'>{</span><span class='hljl-n'>NTK</span><span class='hljl-p'>,</span><span class='hljl-n'>NTVT</span><span class='hljl-p'>})</span><span class='hljl-t'> </span><span class='hljl-n'>where</span><span class='hljl-t'> </span><span class='hljl-p'>{</span><span class='hljl-t'> </span><span class='hljl-n'>NTK</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>T</span><span class='hljl-oB'>&lt;:</span><span class='hljl-n'>Real</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>N1</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>A</span><span class='hljl-oB'>&lt;:</span><span class='hljl-nf'>AbstractArray</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>,</span><span class='hljl-n'>N1</span><span class='hljl-p'>},</span><span class='hljl-t'> </span><span class='hljl-n'>N2</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>NTVT</span><span class='hljl-oB'>&lt;:</span><span class='hljl-nf'>NTuple</span><span class='hljl-p'>{</span><span class='hljl-n'>N2</span><span class='hljl-p'>,</span><span class='hljl-n'>A</span><span class='hljl-p'>}</span><span class='hljl-t'> </span><span class='hljl-p'>}</span><span class='hljl-t'>
    </span><span class='hljl-p'>(;</span><span class='hljl-t'> </span><span class='hljl-nf'>zip</span><span class='hljl-p'>(</span><span class='hljl-nf'>keys</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>convert</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>,</span><span class='hljl-n'>N1</span><span class='hljl-p'>},</span><span class='hljl-nf'>values</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-oB'>...</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
convert_namedtuple_of_arrays_to_namedtuple_of_cuarrays &#40;generic function wi
th 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>obs_data_gpu</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>convert_namedtuple_of_arrays_to_namedtuple_of_cuarrays</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
&#40;t &#61; &#91;0.0007346035529236317, 0.0016015198342701216, 0.0018912311988246084, 
0.0025498764738876153, 0.002603382097432031, 0.0027663354056430695, 0.00346
00259828981533, 0.0037211345105318044, 0.003886042757975414, 0.005341653982
807251  …  365.23792227996745, 365.238230847011, 365.23826068586203, 365.23
857828087455, 365.2396187305002, 365.23976356046046, 365.2398145073391, 365
.241517832939, 365.242077510969, 365.24239781180626&#93;, rv &#61; &#91;4.8205834068965
88, 5.841932517082813, 4.243992013030488, 4.683445064606602, 5.462956705070
644, 6.076242192114535, 5.357507764557043, 4.451056293061384, 4.15560299383
4589, 5.622542499997209  …  -1.6492530858126584, -2.6393540954440238, -2.26
073068351907, -1.5819805520958385, -0.4349175609946101, -1.3930447551784924
, -1.960934709734523, -0.5891212844862384, -0.49188217487424546, -0.8066789
091450677&#93;, σ &#61; &#91;1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 
1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0&#93;&#41;
</pre>


<p>2h.  What do you predict for the time required to compute the predicted velocities &#40;using a pre-allocated workspace on the GPU&#41;, to compute chi-squared on the GPU and to return just the chi-squared value to the CPU?  </p>


<pre class='hljl'>
<span class='hljl-n'>response_2h</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_chisq_kepler</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_data_gpu</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>output_gpu_d64</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 1784 samples with 1 evaluation.
 Range &#40;min … max&#41;:  539.509 μs …  17.694 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 42.
59&#37;
 Time  &#40;median&#41;:     543.968 μs               ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   554.824 μs ± 406.066 μs  ┊ GC &#40;mean ± σ&#41;:  0.76&#37; ±  1.
01&#37;

     ▃▅▆▆▇███▇▇▆▅▅▃▁▂ ▁                        ▁                ▂
  ▇▅███████████████████▇██▇▇▆▄▇▅▅▄▁▁▁▁▁▁▄▇▆▇▇███▇▅▅▇▆▄▇▅▆▆▆▄▄▁▄ █
  540 μs        Histogram: log&#40;frequency&#41; by time        566 μs &lt;

 Memory estimate: 11.50 KiB, allocs estimate: 201.
</pre>


<p>2i.  How did the benchmarking results compare to your prediction?  What might explain any differences?</p>


<pre class='hljl'>
<span class='hljl-n'>response_2i</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>


<p>For examples of how to fuse parallel calculations and reductions into a single GPU kernel call using FLoops, see <a href="https://juliafolds.github.io/FoldsCUDA.jl/dev/examples/inplace_mutation_with_referenceables/#Fusing-reduction-and-mutationg">this example</a>.</p>

<h2>Evaluating many models at once</h2>
<p>In the previous examples, we got a good performance speed-up when we computed a million predicted velocities in one GPU call.  Most stars won&#39;t have nearly that many observations.  If we only had a few hundred observations and used the code above, we would not get nearly as good performance out of the CPU.  &#40;Feel free to try repeating the above benchmarks, changing n_obs.  Remember, you&#39;d need to rerun all the cells that are affected.&#41;  Is there a way that GPU computing might still be useful?  </p>
<p>Typically, we don&#39;t just want to evalute one model, but rather need to evaluate thousand, millions or billions of models to find those that are a good match to the observations.  This creates a second opportunity for parallelization.  We can have each thread evaluate the predicted velocity for one observation time and one set of model parameters.  Let&#39;s try that.  </p>
<p>First, we&#39;ll generate a smaller dataset for testing, and load it onto the GPU.</p>


<pre class='hljl'>
<span class='hljl-n'>n_obs_small</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>256</span><span class='hljl-t'>
</span><span class='hljl-n'>obs_data_small</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>generate_obs_data</span><span class='hljl-p'>(</span><span class='hljl-n'>time_span</span><span class='hljl-oB'>=</span><span class='hljl-n'>time_span_in_years</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>num_obs</span><span class='hljl-oB'>=</span><span class='hljl-n'>n_obs_small</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>param</span><span class='hljl-oB'>=</span><span class='hljl-n'>θ_true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>σ_obs</span><span class='hljl-oB'>=</span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>model</span><span class='hljl-oB'>=</span><span class='hljl-n'>calc_rv_kepler</span><span class='hljl-p'>);</span><span class='hljl-t'>
</span><span class='hljl-n'>obs_data_small_gpu</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>convert_namedtuple_of_arrays_to_namedtuple_of_cuarrays</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data_small</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
&#40;t &#61; &#91;4.157563898077969, 8.148224326943222, 8.261514148192878, 8.2909838354
34867, 8.45441626765078, 8.77154919562908, 8.928478467588857, 10.0135989269
7608, 11.140766193226534, 11.941672888510695  …  357.0877044642848, 357.701
8173820263, 357.8687404461503, 358.98144822031514, 359.3211893095515, 359.3
7889281176143, 361.5076025686745, 362.66409931745113, 364.2364000406141, 36
5.1017334828287&#93;, rv &#61; &#91;-3.016671028778706, -0.09685300033376598, -0.014314
638247165945, 0.09274987634654563, 0.6025538553284715, 3.90025079000391, 3.
844231646389855, -2.564556535813786, -1.127625278169171, 4.893232125384039 
 …  8.67832394564311, -0.8854583736821389, -2.0298357006940084, -1.82016698
3781643, -0.160874323942291, 0.3873792878549537, -3.205628601610107, 1.1347
59993280042, -2.89723068253365, -3.403976804291345&#93;, σ &#61; &#91;1.0, 1.0, 1.0, 1.
0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
 1.0, 1.0&#93;&#41;
</pre>


<p>Now, let&#39;s generate a lot of different sets of model parameters to try evaluating.</p>


<pre class='hljl'>
<span class='hljl-n'>num_models</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>2048</span><span class='hljl-t'> 
</span><span class='hljl-n'>σ_P</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>P_true</span><span class='hljl-oB'>*</span><span class='hljl-nfB'>1e-4</span><span class='hljl-t'>
</span><span class='hljl-n'>σ_K</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>K_true</span><span class='hljl-oB'>*</span><span class='hljl-nfB'>1e-2</span><span class='hljl-t'>
</span><span class='hljl-n'>σ_e</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.1</span><span class='hljl-t'>
</span><span class='hljl-n'>σ_ω</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-n'>π</span><span class='hljl-oB'>*</span><span class='hljl-nfB'>0.1</span><span class='hljl-t'>
</span><span class='hljl-n'>σ_M0</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-n'>π</span><span class='hljl-oB'>*</span><span class='hljl-nfB'>0.1</span><span class='hljl-t'>
</span><span class='hljl-n'>θ_eval</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(;</span><span class='hljl-n'>P</span><span class='hljl-oB'>=</span><span class='hljl-n'>clamp</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-n'>P_true</span><span class='hljl-oB'>.+</span><span class='hljl-n'>σ_P</span><span class='hljl-oB'>.*</span><span class='hljl-nf'>randn</span><span class='hljl-p'>(</span><span class='hljl-n'>num_models</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>Inf</span><span class='hljl-p'>),</span><span class='hljl-t'>
    </span><span class='hljl-n'>K</span><span class='hljl-oB'>=</span><span class='hljl-n'>clamp</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-n'>K_true</span><span class='hljl-oB'>.+</span><span class='hljl-n'>σ_K</span><span class='hljl-oB'>.*</span><span class='hljl-nf'>randn</span><span class='hljl-p'>(</span><span class='hljl-n'>num_models</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>Inf</span><span class='hljl-p'>),</span><span class='hljl-t'>
    </span><span class='hljl-n'>e</span><span class='hljl-oB'>=</span><span class='hljl-n'>clamp</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-n'>e_true</span><span class='hljl-oB'>.+</span><span class='hljl-n'>σ_e</span><span class='hljl-oB'>.*</span><span class='hljl-nf'>randn</span><span class='hljl-p'>(</span><span class='hljl-n'>num_models</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>),</span><span class='hljl-t'>
    </span><span class='hljl-n'>ω</span><span class='hljl-oB'>=</span><span class='hljl-n'>ω_true</span><span class='hljl-oB'>.+</span><span class='hljl-n'>σ_ω</span><span class='hljl-oB'>.*</span><span class='hljl-nf'>randn</span><span class='hljl-p'>(</span><span class='hljl-n'>num_models</span><span class='hljl-p'>),</span><span class='hljl-t'>
    </span><span class='hljl-n'>M0</span><span class='hljl-oB'>=</span><span class='hljl-n'>M0_true</span><span class='hljl-oB'>.+</span><span class='hljl-n'>σ_M0</span><span class='hljl-oB'>.*</span><span class='hljl-nf'>randn</span><span class='hljl-p'>(</span><span class='hljl-n'>num_models</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-p'>);</span>
</pre>



<p>Next, we&#39;ll write a custom kernel that computes the predicted velocity for one observation time and one set of model parameters.   We&#39;ll need to specify a problem size that is a tuple, with the first dimension being the number of times and the second dimension being the number of models to evaluate.</p>


<pre class='hljl'>
<span class='hljl-n'>KernelAbstractions</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@kernel</span><span class='hljl-t'> </span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_kepler_kernel_many_models</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nd'>@Const</span><span class='hljl-p'>(</span><span class='hljl-n'>times</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nd'>@Const</span><span class='hljl-p'>(</span><span class='hljl-n'>P</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nd'>@Const</span><span class='hljl-p'>(</span><span class='hljl-n'>K</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nd'>@Const</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nd'>@Const</span><span class='hljl-p'>(</span><span class='hljl-n'>ω</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nd'>@Const</span><span class='hljl-p'>(</span><span class='hljl-n'>M0</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>I</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>J</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nd'>@index</span><span class='hljl-p'>(</span><span class='hljl-n'>Global</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>NTuple</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>t_I</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>times</span><span class='hljl-p'>[</span><span class='hljl-n'>I</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-n'>param_J</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(;</span><span class='hljl-t'> </span><span class='hljl-n'>P</span><span class='hljl-oB'>=</span><span class='hljl-n'>P</span><span class='hljl-p'>[</span><span class='hljl-n'>J</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>K</span><span class='hljl-oB'>=</span><span class='hljl-n'>K</span><span class='hljl-p'>[</span><span class='hljl-n'>J</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>e</span><span class='hljl-oB'>=</span><span class='hljl-n'>ecc</span><span class='hljl-p'>[</span><span class='hljl-n'>J</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>ω</span><span class='hljl-oB'>=</span><span class='hljl-n'>ω</span><span class='hljl-p'>[</span><span class='hljl-n'>J</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>M0</span><span class='hljl-oB'>=</span><span class='hljl-n'>M0</span><span class='hljl-p'>[</span><span class='hljl-n'>J</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>y</span><span class='hljl-p'>[</span><span class='hljl-n'>I</span><span class='hljl-p'>,</span><span class='hljl-n'>J</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_kepler</span><span class='hljl-p'>(</span><span class='hljl-n'>t_I</span><span class='hljl-p'>,</span><span class='hljl-n'>param</span><span class='hljl-oB'>=</span><span class='hljl-n'>param_J</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
calc_rv_kepler_kernel_many_models &#40;generic function with 5 methods&#41;
</pre>


<p>We&#39;ll create a CPU version of the kernel,  allocate the memory for it to store its results in, and test the CPU kernel.</p>


<pre class='hljl'>
<span class='hljl-n'>cpu_kernel_calc_rv_kepler_many_models!</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_kepler_kernel_many_models</span><span class='hljl-p'>(</span><span class='hljl-nf'>CPU</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-ni'>16</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
KernelAbstractions.Kernel&#123;KernelAbstractions.CPU, KernelAbstractions.NDIter
ation.StaticSize&#123;&#40;16,&#41;&#125;, KernelAbstractions.NDIteration.DynamicSize, typeof
&#40;Main.##WeaveSandBox#257.cpu_calc_rv_kepler_kernel_many_models&#41;&#125;&#40;Main.##Wea
veSandBox#257.cpu_calc_rv_kepler_kernel_many_models&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>output_many_models_cpu</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>zeros</span><span class='hljl-p'>(</span><span class='hljl-n'>n_obs_small</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>num_models</span><span class='hljl-p'>);</span>
</pre>




<pre class='hljl'>
<span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-nf'>cpu_kernel_calc_rv_kepler_many_models!</span><span class='hljl-p'>(</span><span class='hljl-n'>output_many_models_cpu</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>obs_data_small</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_eval</span><span class='hljl-oB'>.</span><span class='hljl-n'>P</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_eval</span><span class='hljl-oB'>.</span><span class='hljl-n'>K</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_eval</span><span class='hljl-oB'>.</span><span class='hljl-n'>e</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_eval</span><span class='hljl-oB'>.</span><span class='hljl-n'>ω</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_eval</span><span class='hljl-oB'>.</span><span class='hljl-n'>M0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-p'>(</span><span class='hljl-t'> </span><span class='hljl-n'>n_obs_small</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>num_models</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-p'>))</span>
</pre>



<p>Now, we&#39;ll create a GPU version of the kernel, allocate the memory for it to store its results in, load the model parameters to be evaluated onto the GPU, and test Ghe CPU kernel.</p>


<pre class='hljl'>
<span class='hljl-n'>gpu_kernel_calc_rv_kepler_many_models!</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_rv_kepler_kernel_many_models</span><span class='hljl-p'>(</span><span class='hljl-n'>CUDAKernels</span><span class='hljl-oB'>.</span><span class='hljl-nf'>CUDADevice</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-ni'>32</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
KernelAbstractions.Kernel&#123;CUDAKernels.CUDADevice, KernelAbstractions.NDIter
ation.StaticSize&#123;&#40;32,&#41;&#125;, KernelAbstractions.NDIteration.DynamicSize, typeof
&#40;Main.##WeaveSandBox#257.gpu_calc_rv_kepler_kernel_many_models&#41;&#125;&#40;Main.##Wea
veSandBox#257.gpu_calc_rv_kepler_kernel_many_models&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>output_many_models_gpu</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>zeros</span><span class='hljl-p'>(</span><span class='hljl-n'>Float64</span><span class='hljl-p'>,</span><span class='hljl-n'>n_obs_small</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>num_models</span><span class='hljl-p'>);</span>
</pre>




<pre class='hljl'>
<span class='hljl-n'>θ_eval_gpu</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>convert_namedtuple_of_arrays_to_namedtuple_of_cuarrays</span><span class='hljl-p'>(</span><span class='hljl-n'>θ_eval</span><span class='hljl-p'>);</span>
</pre>




<pre class='hljl'>
<span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-nf'>gpu_kernel_calc_rv_kepler_many_models!</span><span class='hljl-p'>(</span><span class='hljl-n'>output_many_models_gpu</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>obs_data_small_gpu</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_eval_gpu</span><span class='hljl-oB'>.</span><span class='hljl-n'>P</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_eval_gpu</span><span class='hljl-oB'>.</span><span class='hljl-n'>K</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_eval_gpu</span><span class='hljl-oB'>.</span><span class='hljl-n'>e</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_eval_gpu</span><span class='hljl-oB'>.</span><span class='hljl-n'>ω</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_eval_gpu</span><span class='hljl-oB'>.</span><span class='hljl-n'>M0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-p'>(</span><span class='hljl-t'> </span><span class='hljl-n'>n_obs_small</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>num_models</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-p'>))</span>
</pre>



<p>A quick check that the results are consistent given expected limitations of floating point arithmetic.</p>


<pre class='hljl'>
<span class='hljl-nf'>maximum</span><span class='hljl-p'>(</span><span class='hljl-n'>abs</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-n'>output_many_models_gpu</span><span class='hljl-p'>)</span><span class='hljl-oB'>.-</span><span class='hljl-n'>output_many_models_cpu</span><span class='hljl-p'>))</span>
</pre>


<pre class="output">
9.325873406851315e-15
</pre>


<p>2j. What do you predict for the speed-up factor of the GPU relative to the CPU?</p>


<pre class='hljl'>
<span class='hljl-n'>response_2j</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-nf'>cpu_kernel_calc_rv_kepler_many_models!</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>output_many_models_cpu</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_data_small</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_eval</span><span class='hljl-oB'>.</span><span class='hljl-n'>P</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_eval</span><span class='hljl-oB'>.</span><span class='hljl-n'>K</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_eval</span><span class='hljl-oB'>.</span><span class='hljl-n'>e</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_eval</span><span class='hljl-oB'>.</span><span class='hljl-n'>ω</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_eval</span><span class='hljl-oB'>.</span><span class='hljl-n'>M0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-p'>(</span><span class='hljl-t'> </span><span class='hljl-n'>n_obs_small</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>num_models</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 26 samples with 1 evaluation.
 Range &#40;min … max&#41;:  38.511 ms … 45.273 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 0.00&#37;
 Time  &#40;median&#41;:     38.690 ms              ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   38.986 ms ±  1.304 ms  ┊ GC &#40;mean ± σ&#41;:  0.00&#37; ± 0.00&#37;

  █▅▃                                                          
  ███▄▄▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄ ▁
  38.5 ms         Histogram: frequency by time        45.3 ms &lt;

 Memory estimate: 4.22 KiB, allocs estimate: 66.
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-nf'>gpu_kernel_calc_rv_kepler_many_models!</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>output_many_models_gpu</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_data_small_gpu</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_eval_gpu</span><span class='hljl-oB'>.</span><span class='hljl-n'>P</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_eval_gpu</span><span class='hljl-oB'>.</span><span class='hljl-n'>K</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_eval_gpu</span><span class='hljl-oB'>.</span><span class='hljl-n'>e</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_eval_gpu</span><span class='hljl-oB'>.</span><span class='hljl-n'>ω</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_eval_gpu</span><span class='hljl-oB'>.</span><span class='hljl-n'>M0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-p'>(</span><span class='hljl-t'> </span><span class='hljl-n'>n_obs_small</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>num_models</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 1781 samples with 1 evaluation.
 Range &#40;min … max&#41;:  441.089 μs … 714.003 μs  ┊ GC &#40;min … max&#41;: 0.00&#37; … 0.0
0&#37;
 Time  &#40;median&#41;:     544.519 μs               ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   547.952 μs ±  43.841 μs  ┊ GC &#40;mean ± σ&#41;:  0.00&#37; ± 0.0
0&#37;

                        █▃    ▄▂                                 
  ▃▂▂▁▂▂▂▁▃▅▆▅▃▃▃▄▅▄▅▄▃▆███▆▅▄██▇▇▆▄▅▅▄▄▄▄▃▄▆▅▅▃▃▄▄▃▃▃▃▂▃▃▂▂▂▃▃ ▃
  441 μs           Histogram: frequency by time          664 μs &lt;

 Memory estimate: 4.42 KiB, allocs estimate: 82.
</pre>


<p>2k.  How did the benchmarking results compare to your prediction?  What might explain any differences?</p>


<pre class='hljl'>
<span class='hljl-n'>response_2k</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>


<h2>Evaluating many models with reduction</h2>
<p>We can likely get a further speed-up be performing the reduction operation on the GPU and returning only the goodness of fit statistics, rather than every predicted velocity.  We&#39;ll try that below.</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>cpu_calc_chisq_kepler_many_models</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>rv_obs</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>σ</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nf'>ismissing</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>||</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-nf'>first</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>rv_pred</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>ismissing</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-nf'>zeros</span><span class='hljl-p'>(</span><span class='hljl-nf'>eltype</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>),</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>),</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-nf'>first</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-p'>)))</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'>
    </span><span class='hljl-nf'>wait</span><span class='hljl-p'>(</span><span class='hljl-nf'>cpu_kernel_calc_rv_kepler_many_models!</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_pred</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-oB'>.</span><span class='hljl-n'>P</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-oB'>.</span><span class='hljl-n'>K</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-oB'>.</span><span class='hljl-n'>e</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-oB'>.</span><span class='hljl-n'>ω</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-oB'>.</span><span class='hljl-n'>M0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-p'>(</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-nf'>first</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-p'>)))</span><span class='hljl-t'> </span><span class='hljl-p'>))</span><span class='hljl-t'>
    </span><span class='hljl-n'>χ²</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>vec</span><span class='hljl-p'>(</span><span class='hljl-nf'>sum</span><span class='hljl-p'>(((</span><span class='hljl-n'>rv_pred</span><span class='hljl-oB'>.-</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-oB'>./</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-oB'>.^</span><span class='hljl-ni'>2</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>dims</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span><span class='hljl-p'>))</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>cpu_calc_chisq_kepler_many_models</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>::</span><span class='hljl-n'>NamedTuple</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nf'>cpu_calc_chisq_kepler_many_models</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>rv</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>σ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
cpu_calc_chisq_kepler_many_models &#40;generic function with 2 methods&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_chisq_kepler_many_models</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-oB'>::</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>rv_obs</span><span class='hljl-oB'>::</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>σ</span><span class='hljl-oB'>::</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-oB'>::</span><span class='hljl-nf'>Union</span><span class='hljl-p'>{</span><span class='hljl-n'>Missing</span><span class='hljl-p'>,</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>}</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>zeros</span><span class='hljl-p'>(</span><span class='hljl-nf'>eltype</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>),</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>),</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-oB'>.</span><span class='hljl-n'>P</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nf'>ismissing</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>||</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-nf'>first</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>rv_pred</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>ismissing</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>zeros</span><span class='hljl-p'>(</span><span class='hljl-nf'>eltype</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>),</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>),</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-nf'>first</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-p'>)))</span><span class='hljl-t'>  </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'>
    </span><span class='hljl-p'>(</span><span class='hljl-nf'>gpu_kernel_calc_rv_kepler_many_models!</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_pred</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-oB'>.</span><span class='hljl-n'>P</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-oB'>.</span><span class='hljl-n'>K</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-oB'>.</span><span class='hljl-n'>e</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-oB'>.</span><span class='hljl-n'>ω</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-oB'>.</span><span class='hljl-n'>M0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ndrange</span><span class='hljl-oB'>=</span><span class='hljl-p'>(</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-nf'>first</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-p'>)))</span><span class='hljl-t'> </span><span class='hljl-p'>))</span><span class='hljl-t'>
    </span><span class='hljl-n'>χ²</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>vec</span><span class='hljl-p'>(</span><span class='hljl-nf'>sum</span><span class='hljl-p'>(((</span><span class='hljl-n'>rv_pred</span><span class='hljl-oB'>.-</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-oB'>./</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-oB'>.^</span><span class='hljl-ni'>2</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>dims</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span><span class='hljl-p'>))</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_chisq_kepler_many_models</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>rv_obs</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>σ</span><span class='hljl-oB'>::</span><span class='hljl-n'>AbstractArray</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>t_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>isa</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>t</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-nf'>convert</span><span class='hljl-p'>(</span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>{</span><span class='hljl-n'>Float64</span><span class='hljl-p'>,</span><span class='hljl-ni'>1</span><span class='hljl-p'>},</span><span class='hljl-n'>t</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>rv_obs_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>isa</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>,</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>rv_obs</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-nf'>convert</span><span class='hljl-p'>(</span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>{</span><span class='hljl-n'>Float64</span><span class='hljl-p'>,</span><span class='hljl-ni'>1</span><span class='hljl-p'>},</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>σ_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>isa</span><span class='hljl-p'>(</span><span class='hljl-n'>σ</span><span class='hljl-p'>,</span><span class='hljl-n'>CuArray</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>σ</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-nf'>convert</span><span class='hljl-p'>(</span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>{</span><span class='hljl-n'>Float64</span><span class='hljl-p'>,</span><span class='hljl-ni'>1</span><span class='hljl-p'>},</span><span class='hljl-n'>σ</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>workspace_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>isa</span><span class='hljl-p'>(</span><span class='hljl-n'>workspace</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>CuArray</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>zeros</span><span class='hljl-p'>(</span><span class='hljl-nf'>eltype</span><span class='hljl-p'>(</span><span class='hljl-n'>rv_obs</span><span class='hljl-p'>),</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>t_d</span><span class='hljl-p'>),</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-nf'>first</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-p'>)))</span><span class='hljl-t'>
    </span><span class='hljl-nf'>gpu_calc_chisq_kepler_many_models</span><span class='hljl-p'>(</span><span class='hljl-n'>t_d</span><span class='hljl-p'>,</span><span class='hljl-n'>rv_obs_d</span><span class='hljl-p'>,</span><span class='hljl-n'>σ_d</span><span class='hljl-p'>,</span><span class='hljl-n'>θ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-oB'>=</span><span class='hljl-n'>workspace_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_chisq_kepler_many_models</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>::</span><span class='hljl-n'>NamedTuple</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nf'>gpu_calc_chisq_kepler_many_models</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>rv</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-n'>σ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_calc_chisq_kepler_many_models &#40;generic function with 3 methods&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>χ²s_gpu</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_chisq_kepler_many_models</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data_small_gpu</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_eval_gpu</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>output_many_models_gpu</span><span class='hljl-p'>);</span><span class='hljl-t'>
</span><span class='hljl-n'>χ²s_cpu</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>cpu_calc_chisq_kepler_many_models</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data_small</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>θ_eval</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>output_many_models_cpu</span><span class='hljl-p'>);</span><span class='hljl-t'>
</span><span class='hljl-nf'>maximum</span><span class='hljl-p'>(</span><span class='hljl-n'>abs</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-n'>χ²s_gpu</span><span class='hljl-p'>)</span><span class='hljl-oB'>.-</span><span class='hljl-n'>χ²s_cpu</span><span class='hljl-p'>))</span>
</pre>


<pre class="output">
7.275957614183426e-12
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_chisq_kepler_many_models</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_data_small_gpu</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_eval_gpu</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>output_many_models_gpu</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 2775 samples with 1 evaluation.
 Range &#40;min … max&#41;:   63.185 μs …  39.558 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 0.0
0&#37;
 Time  &#40;median&#41;:     431.230 μs               ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   355.541 μs ± 761.479 μs  ┊ GC &#40;mean ± σ&#41;:  0.00&#37; ± 0.0
0&#37;

  ▁▃                                                     ▂▅█▄    
  ██▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▁▂▁▂▁▂▁▁▁▂▁▂▂▂▂▃████▇▄ ▃
  63.2 μs          Histogram: frequency by time          460 μs &lt;

 Memory estimate: 10.38 KiB, allocs estimate: 162.
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-n'>χ²s_cpu</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>cpu_calc_chisq_kepler_many_models</span><span class='hljl-p'>(</span><span class='hljl-oB'>$</span><span class='hljl-n'>obs_data_small</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>θ_eval</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>workspace</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-oB'>$</span><span class='hljl-n'>output_many_models_cpu</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-n'>seconds</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 25 samples with 1 evaluation.
 Range &#40;min … max&#41;:  39.353 ms … 53.171 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 26.00
&#37;
 Time  &#40;median&#41;:     39.518 ms              ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   40.124 ms ±  2.731 ms  ┊ GC &#40;mean ± σ&#41;:  1.38&#37; ±  5.20
&#37;

  █                                                            
  █▃▃▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃ ▁
  39.4 ms         Histogram: frequency by time        53.2 ms &lt;

 Memory estimate: 4.02 MiB, allocs estimate: 70.
</pre>


<p>2l. What do you observe for the speed-up factor of the GPU relative to the CPU now that we&#39;re performing the reduction on the GPU with a pre-allocated workspace?</p>


<pre class='hljl'>
<span class='hljl-n'>response_2l</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>


<p>2m.  If you still have some time, try changing <code>num_models</code> and rerunning the affected cells.   How many models do you need to evaluate in parallel to get at least a factor of 50 performance improvement over the CPU?</p>


<pre class='hljl'>
<span class='hljl-n'>response_2m</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>missing</span><span class='hljl-t'> </span><span class='hljl-cs'># md&quot;Insert response&quot;</span>
</pre>


<pre class="output">
missing
</pre>


<h3>Comparison to simple CPU version</h3>
<p>Just for fun, here&#39;s a benchmark for the same computation on the CPU without kernel abstraction.</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>get_nth_as_namedtuple</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-oB'>::</span><span class='hljl-nf'>NamedTuple</span><span class='hljl-p'>{</span><span class='hljl-n'>NTK</span><span class='hljl-p'>,</span><span class='hljl-n'>NTVT</span><span class='hljl-p'>},</span><span class='hljl-t'> </span><span class='hljl-n'>n</span><span class='hljl-oB'>::</span><span class='hljl-n'>Integer</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-n'>where</span><span class='hljl-t'> </span><span class='hljl-p'>{</span><span class='hljl-t'> </span><span class='hljl-n'>NTK</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>T</span><span class='hljl-oB'>&lt;:</span><span class='hljl-n'>Real</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>N1</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>A</span><span class='hljl-oB'>&lt;:</span><span class='hljl-nf'>AbstractArray</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>,</span><span class='hljl-n'>N1</span><span class='hljl-p'>},</span><span class='hljl-t'> </span><span class='hljl-n'>N2</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>NTVT</span><span class='hljl-oB'>&lt;:</span><span class='hljl-nf'>NTuple</span><span class='hljl-p'>{</span><span class='hljl-n'>N2</span><span class='hljl-p'>,</span><span class='hljl-n'>A</span><span class='hljl-p'>}</span><span class='hljl-t'> </span><span class='hljl-p'>}</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-n'>n</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-nf'>first</span><span class='hljl-p'>(</span><span class='hljl-nf'>values</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-p'>)))</span><span class='hljl-t'>
    </span><span class='hljl-p'>(;</span><span class='hljl-t'> </span><span class='hljl-nf'>zip</span><span class='hljl-p'>(</span><span class='hljl-nf'>keys</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nf'>map</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-oB'>-&gt;</span><span class='hljl-n'>x</span><span class='hljl-p'>[</span><span class='hljl-n'>n</span><span class='hljl-p'>],</span><span class='hljl-nf'>values</span><span class='hljl-p'>(</span><span class='hljl-n'>θ</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-oB'>...</span><span class='hljl-t'> </span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
get_nth_as_namedtuple &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@elapsed</span><span class='hljl-t'> </span><span class='hljl-nf'>map</span><span class='hljl-p'>(</span><span class='hljl-n'>n</span><span class='hljl-oB'>-&gt;</span><span class='hljl-nf'>calc_chisq_rv_kepler_cpu_simple</span><span class='hljl-p'>(</span><span class='hljl-n'>obs_data_small</span><span class='hljl-p'>,</span><span class='hljl-nf'>get_nth_as_namedtuple</span><span class='hljl-p'>(</span><span class='hljl-n'>θ_eval</span><span class='hljl-p'>,</span><span class='hljl-n'>n</span><span class='hljl-p'>)),</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-nf'>first</span><span class='hljl-p'>(</span><span class='hljl-n'>θ_eval</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
0.336248111
</pre>



<pre class='hljl'>

</pre>




        <HR/>
        <div class="footer">
          <p>
            Published from <a href="ex2.ipynb">ex2.ipynb</a>
            using <a href="http://github.com/JunoLab/Weave.jl">Weave.jl</a> v0.10.10 on 2021-11-03.
          </p>
        </div>
      </div>
    </div>
  </div>
</BODY>

</HTML>
